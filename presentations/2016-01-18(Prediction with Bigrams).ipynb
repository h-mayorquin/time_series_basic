{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with Bigrams\n",
    "In this notebook I will show how to do predictions in text using a very simple scheme. The idea is to get the frequency of the letters for a certain text and for each letter predict the most frequent letter. This method is in my opinion the simplest method that do not use temporal information.\n",
    "\n",
    "In order to obtain a prediction that is as simple and yet includes temporal information to make the prediction we calculate the most frequent bigrams (pair of letters like 'r' and 's' at the end of the word letters). We calculate for every letter what is the letter that is more likely to follow and use that as a prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import text7 as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we extract the information from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters = ' '.join(text)\n",
    "letters = [letter.lower() for letter in letters] # Get the lowercase\n",
    "symbols = set(letters)\n",
    "Nletters = len(letters)\n",
    "Nsymbols = len(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the frequency for all the letters and the most common which turns out to be a space. Latter we will analyze how the result changes when we remove space from the whole analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common letter  \n"
     ]
    }
   ],
   "source": [
    "freq_letters = nltk.FreqDist(letters) # Get the most frequent letters\n",
    "most_common_letter = freq_letters.most_common(1)[0][0]\n",
    "print('most common letter', most_common_letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will the bigrams frequency as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(letters)\n",
    "freq_bigrams = nltk.FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to extract the next most probable letter for every letter. From the bigran frequency first we make a dictionary (master dictionary ) of all the next letters and their frequency for each letter (each symbol here). In particular we use a dictionary where the key is the symbol and the value is a list with a the tuples of the next letter and its frequency. With this in our hand we take for every list the one with the maximun frequency using a lambda function and build the next letter dictionary with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_dictionary = {}\n",
    "next_letters = {}\n",
    "\n",
    "for symbol in symbols:\n",
    "    master_dictionary[symbol] = [(key[1], value) for key,value in freq_bigrams.items() if key[0]==symbol]\n",
    "\n",
    "for symbol in symbols:\n",
    "    aux = max(master_dictionary[symbol], key=lambda x:x[1])  # Maximize over the second element of the tuple\n",
    "    next_letters[symbol] = aux[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction not using time 18.497287187034352\n"
     ]
    }
   ],
   "source": [
    "prediction = 0\n",
    "for letter in letters:\n",
    "    if letter == most_common_letter:\n",
    "        prediction += 1  # Get's the result right\n",
    "\n",
    "prediction /= Nletters\n",
    "print('Prediction not using time', prediction * 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using temporal information 27.537486059283182\n"
     ]
    }
   ],
   "source": [
    "# Now we make use of the temporal information\n",
    "prediction_temp = 0\n",
    "last_letter = None\n",
    "for index, letter in enumerate(letters):\n",
    "    if last_letter:  # If last_letter is not None\n",
    "        if next_letters[last_letter] == letter:\n",
    "            prediction_temp += 1\n",
    "    # Save the last letter\n",
    "    last_letter = letter\n",
    "\n",
    "prediction_temp /= Nletters\n",
    "print('Prediction using temporal information', prediction_temp * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
